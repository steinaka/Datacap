---
title: "Capstone project"
output: html_document
Author: KX
Date : December 2019
---


### Introduction
This is the  milestone report for Week 2 in Data Science Capstone project. 

#### Objective 

To explain the Explortory Data Analysis data and following which the eventual prediction app and algorithm. Users will provide a word or a phrase and the application will try to predict the next word. The model will be trained using a corpus (a collection of English text) that is compiled from 3 sources - news, blogs, and tweets. In the following report, I load and clean the data as well as use NLM (Natural Language Processing) applications in R (tm and RWeka) to tokenize n-grams as a first step toward building a predictive model.




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Preload necessary R libary and parameters setup
```{r, include=FALSE}
library(rJava)
library(knitr)
library(RColorBrewer)
library(stringi)
library(wordcloud2)
library(ggplot2)
library(ngram)
library(NLP)
library(tm)
library(RWeka)
library(slam)
library(xtable)
library(downloader)
library(plyr)
library(dplyr)
library(doParallel)
library(SnowballC)
library(wordcloud)


options(mc.cores=1)
```

## Step 1: Download the dataset and unzip folder
## Check if directory already exists

```{r}

if(!file.exists("./final")){
  dir.create("./final")
}
Url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
## Check if zip has already been downloaded in projectData directory?
if(!file.exists("./final/Coursera-SwiftKey.zip")){
  download.file(Url,destfile="./final/Coursera-SwiftKey.zip",mode = "wb")
}
## Check if zip has already been unzipped?
if(file.exists("./final")){
  unzip(zipfile="./final/Coursera-SwiftKey.zip",exdir=".")
}
# Once the dataset is downloaded start reading it as this a huge dataset so we'll read line by line only the amount of data needed before doing that lets first list all the files in the directory
path <- file.path("./final" , "en_US")
files<-list.files(path, recursive=TRUE)
# Lets make a file connection of the twitter data set
con <- file("./final/en_US/en_US.twitter.txt", "r") 
#lineTwitter<-readLines(con,encoding = "UTF-8", skipNul = TRUE)
lineTwitter<-readLines(con, skipNul = TRUE)
# Close the connection handle when you are done
close(con)
# Lets make a file connection of the blog data set
con <- file("./final/en_US/en_US.blogs.txt", "r") 
#lineBlogs<-readLines(con,encoding = "UTF-8", skipNul = TRUE)
lineBlogs<-readLines(con, skipNul = TRUE)
# Close the connection handle when you are done
close(con)
# Lets make a file connection of the news data set
con <- file("./final/en_US/en_US.news.txt", "r") 
#lineNews<-readLines(con,encoding = "UTF-8", skipNul = TRUE)
lineNews<-readLines(con, skipNul = TRUE)
# Close the connection handle when you are done
close(con)
# Get file sizes
lineBlogs.size <- file.info("./final/en_US/en_US.blogs.txt")$size / 1024 ^ 2
lineNews.size <- file.info("./final/en_US/en_US.news.txt")$size / 1024 ^ 2
lineTwitter.size <- file.info("./final/en_US/en_US.twitter.txt")$size / 1024 ^ 2


```


# Get words in files

```{r }

lineBlogs.words <- stri_count_words(lineBlogs)
lineNews.words <- stri_count_words(lineNews)
lineTwitter.words <- stri_count_words(lineTwitter)
```


# Summary of the data sets

```{r}
data.frame(source = c("blogs", "news", "twitter"),
           file.size.MB = c(lineBlogs.size, lineNews.size, lineTwitter.size),
           num.lines = c(length(lineBlogs), length(lineNews), length(lineTwitter)),
           num.words = c(sum(lineBlogs.words), sum(lineNews.words), sum(lineTwitter.words)),
           mean.num.words = c(mean(lineBlogs.words), mean(lineNews.words), mean(lineTwitter.words)))


## Display Statistics of data


WPL <- sapply(list(lineBlogs,lineNews,lineTwitter),function(x) summary(stri_count_words(x))[c('Min.','Mean','Max.')])
rownames(WPL) <- c('WPL_Min','WPL_Mean','WPL_Max')
stats <- data.frame(
  FileName=c("en_US.blogs","en_US.news","en_US.twitter"),      
  t(rbind(
    sapply(list(lineBlogs,lineNews,lineTwitter),stri_stats_general)[c('Lines','Chars'),],
    Words=sapply(list(lineBlogs,lineNews,lineTwitter),stri_stats_latex)['Words',],
    WPL)
  ))

head(stats)
head(lineBlogs.size)
head(lineNews.size)
head(lineTwitter.size)
```


## Cleaning The Data

# Sample the data and set seed for reporducibility

Sample seed of 5000 is chosen.

```{r , include=FALSE}
set.seed(5000)
data.sample <- c(sample(lineBlogs, length(lineBlogs) * 0.02),
                 sample(lineNews, length(lineNews) * 0.02),
                 sample(lineTwitter, length(lineTwitter) * 0.02))

```


# Create corpus and clean the data
This is done using the tm_map function. All the random symbols are remoed

```{r , include=FALSE}
corpus <- VCorpus(VectorSource(data.sample))
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus <- tm_map(corpus, toSpace, "(f|ht)tp(s?)://(.*)[.][a-z]+")
corpus <- tm_map(corpus, toSpace, "@[^\\s]+")
corpus <- tm_map(corpus, tolower)
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, PlainTextDocument)
unicorpus <- tm_map(corpus, removeWords, stopwords("en"))
```

##Exploratory Analysis
# we'll get the frequencies of the word

Data set is split into 1 - 6 grams

```{r }
getFreq <- function(tdm) {
  freq <- sort(rowSums(as.matrix(tdm)), decreasing = TRUE)
  return(data.frame(word = names(freq), freq = freq))
}

# Prepare n-gram frequencies
getFreq <- function(tdm) {
  freq <- sort(rowSums(as.matrix(rollup(tdm, 2, FUN = sum)), na.rm = T), decreasing = TRUE)
  return(data.frame(word = names(freq), freq = freq))
}
bigram <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
trigram <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
quadgram <- function(x) NGramTokenizer(x, Weka_control(min = 4, max = 4))
pentagram <- function(x) NGramTokenizer(x, Weka_control(min = 5, max = 5))
hexagram <- function(x) NGramTokenizer(x, Weka_control(min = 6, max = 6))

# Get frequencies of most common n-grams in data sample
freq1 <- getFreq(removeSparseTerms(TermDocumentMatrix(unicorpus), 0.999))
save(freq1, file="nfreq.f1.RData")
freq2 <- getFreq(TermDocumentMatrix(unicorpus, control = list(tokenize = bigram, bounds = list(global = c(5, Inf)))))
save(freq2, file="nfreq.f2.RData")
freq3 <- getFreq(TermDocumentMatrix(corpus, control = list(tokenize = trigram, bounds = list(global = c(3, Inf)))))
save(freq3, file="nfreq.f3.RData")
freq4 <- getFreq(TermDocumentMatrix(corpus, control = list(tokenize = quadgram, bounds = list(global = c(2, Inf)))))
save(freq4, file="nfreq.f4.RData")
freq5 <- getFreq(TermDocumentMatrix(corpus, control = list(tokenize = pentagram, bounds = list(global = c(2, Inf)))))
save(freq5, file="nfreq.f5.RData")
freq6 <- getFreq(TermDocumentMatrix(corpus, control = list(tokenize = hexagram, bounds = list(global = c(2, Inf)))))
save(freq6, file="nfreq.f6.RData")
nf <- list("f1" = freq1, "f2" = freq2, "f3" = freq3, "f4" = freq4, "f5" = freq5, "f6" = freq6)
save(nf, file="nfreq.v5.RData")
```


##------
## Word cloud display 

The word clouds for Uni grams to 6 grams


```{r}
wordcloud(freq1$word, freq1$freq, scale = c(2,.5), max.words=100, random.order=TRUE, rot.per=0, fixed.asp = TRUE, use.r.layout = FALSE, colors=brewer.pal(8, "Dark2"))
```

```{r}
wordcloud(freq2$word, freq2$freq, scale = c(2,.5), max.words=100, random.order=FALSE, rot.per=0, fixed.asp = TRUE, use.r.layout = FALSE, colors=brewer.pal(8, "Dark2"))
```


```{r}
wordcloud(freq3$word, freq3$freq, scale = c(2,.5), max.words=100, random.order=FALSE, rot.per=0, fixed.asp = TRUE, use.r.layout = FALSE, colors=brewer.pal(8, "Dark2"))
```


```{r}
wordcloud(freq4$word, freq4$freq, scale = c(2,.5), max.words=60, random.order=FALSE, rot.per=0, fixed.asp = TRUE, use.r.layout = FALSE, colors=brewer.pal(8, "Dark2"))
```


```{r}
wordcloud(freq5$word, freq5$freq, scale = c(2,.5), max.words=40, random.order=FALSE, rot.per=0, fixed.asp = TRUE, use.r.layout = FALSE, colors=brewer.pal(8, "Dark2"))
```


```{r}
wordcloud(freq6$word, freq6$freq, scale = c(1,.1), max.words=100, random.order=FALSE, rot.per=0, fixed.asp = TRUE, use.r.layout = FALSE, colors=brewer.pal(8, "Dark2"))
```

## Plot Histogram


```{r}

ggplot(data = head(freq1, 20), aes(x = reorder(word, -freq), y = freq)) + 
    geom_bar(stat = "identity", fill = "green") + 
    ggtitle(paste("1grams")) + 
    xlab("Unigrams") + ylab("Frequency") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

```
  
  
```{r}

ggplot(data = head(freq2, 20), aes(x = reorder(word, -freq), y = freq)) + 
    geom_bar(stat = "identity", fill = "green") + 
    ggtitle(paste("2grams")) + 
    xlab("2grams") + ylab("Frequency") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

```


  
```{r}

ggplot(data = head(freq3, 20), aes(x = reorder(word, -freq), y = freq)) + 
    geom_bar(stat = "identity", fill = "green") + 
    ggtitle(paste("3grams")) + 
    xlab("3grams") + ylab("Frequency") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

```


  
```{r}

ggplot(data = head(freq4, 20), aes(x = reorder(word, -freq), y = freq)) + 
    geom_bar(stat = "identity", fill = "green") + 
    ggtitle(paste("4grams")) + 
    xlab("4grams") + ylab("Frequency") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

```


  
```{r}

ggplot(data = head(freq5, 20), aes(x = reorder(word, -freq), y = freq)) + 
    geom_bar(stat = "identity", fill = "green") + 
    ggtitle(paste("5grams")) + 
    xlab("5grams") + ylab("Frequency") + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))

```


  
```{r}

ggplot(data = head(freq6, 20), aes(x = reorder(word, -freq), y = freq)) + 
    geom_bar(stat = "identity", fill = "green") + 
    ggtitle(paste("6grams")) + 
    xlab("6grams") + ylab("Frequency") + 
    theme(axis.text.x = element_text(angle = 45, hjust = 1,size = 7))

```


